{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14926842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Oracle_Assignment_2 as oa\n",
    "# import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# oa.q2_get_mnist_jpg_subset(23634)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c3da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2c6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure grayscale\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b085d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"q2_data\", transform=transform)\n",
    "\n",
    "# Split dataset into training and test sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6615895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad() :\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs, test_loader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update weights using SGD with momentum\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # evaluate(model, train_loader, test_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "def metricslearn(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.numpy().reshape(images.shape[0], -1)\n",
    "        data_list.append(images)\n",
    "        labels_list.append(labels.numpy())\n",
    "\n",
    "    data_array = np.vstack(data_list)\n",
    "    labels_array = np.concatenate(labels_list)\n",
    "    return data_array, labels_array\n",
    "\n",
    "train_data, train_labels = dataloader_to_numpy(train_loader)\n",
    "test_data, test_labels = dataloader_to_numpy(test_loader)\n",
    "\n",
    "\n",
    "def pca(data, k) :\n",
    "    cov_matrix = np.cov(data.T)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvecks = eigenvectors[:, :k]\n",
    "    new_data = np.dot(data, eigenvecks)\n",
    "    return new_data, eigenvecks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10463a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eb65a20fcb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mean = np.mean(train_data, axis=0)\n",
    "std = np.std(train_data, axis=0)\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "k = 100\n",
    "train_data_pca, eigenvecks = pca(train_data, k)\n",
    "test_data_pca = np.dot(test_data, eigenvecks)\n",
    "\n",
    "\n",
    "\n",
    "# Convert PCA-transformed data to tensors\n",
    "train_data_pca_tensor = torch.tensor(train_data_pca, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_data_pca_tensor = torch.tensor(test_data_pca, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_pca_dataset = TensorDataset(train_data_pca_tensor, train_labels_tensor)\n",
    "test_pca_dataset = TensorDataset(test_data_pca_tensor, test_labels_tensor)\n",
    "\n",
    "train_pca_loader = DataLoader(train_pca_dataset, batch_size=32, shuffle=True)\n",
    "test_pca_loader = DataLoader(test_pca_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Reconstruct a sample image\n",
    "sample = train_data_pca[10]\n",
    "reconstructed = np.dot(sample, eigenvecks.T)\n",
    "reconstructed = (reconstructed * std) + mean\n",
    "reconstructed = reconstructed.reshape(28, 28)\n",
    "plt.imshow(reconstructed, cmap=\"gray\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01f0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "from kan import *\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = KAN(width=[100,10,20,10],grid =5 ,k=3,seed=42,device=device).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd45a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.8410, -3.5703, -4.1523, -9.2323,  3.2586,  3.2736, -0.4720,  1.7042,\n",
      "        -5.1720, -6.2205, -5.2815, -0.6185,  0.1433, -1.6192, -2.3616, -0.0113,\n",
      "        -0.1836, -0.1510, -0.8277,  2.0887,  0.6677,  0.7742,  1.7195,  1.8318,\n",
      "        -0.7935, -0.6966,  2.6199,  3.5512,  1.0218,  2.7797,  1.0196, -1.7061,\n",
      "        -3.8021,  1.2898,  2.6860, -1.1304,  0.9435,  0.5946,  1.2168, -0.4166,\n",
      "         1.7943,  0.1758,  1.4968, -2.2573, -1.3422, -0.5738, -2.0050,  0.7454,\n",
      "        -1.3775, -1.1532, -2.3072,  1.7029, -1.8015, -3.2567, -0.4191,  1.2417,\n",
      "        -0.2546,  0.4208,  1.0112,  0.2721, -0.3679,  2.1983,  0.3614, -1.7663,\n",
      "        -0.9487, -1.9230, -0.6400, -1.1878,  1.4066,  0.3213, -0.4821,  0.3734,\n",
      "        -0.0194,  0.0687,  0.5277,  0.4679, -1.6277, -0.7312, -0.0867,  0.6921,\n",
      "        -1.9358, -0.0751, -1.3049, -1.0311,  0.2768,  1.3655, -0.3048,  2.4794,\n",
      "        -0.8698,  1.3364, -0.2179, -1.1997,  0.8484, -0.6041,  1.1325, -0.1296,\n",
      "         0.1251,  1.7323, -0.4565, -0.0132]), tensor(6))\n",
      "Epoch [1/17], Loss: 1.4863\n",
      "Epoch [2/17], Loss: 0.6862\n",
      "Epoch [3/17], Loss: 0.5786\n",
      "Epoch [4/17], Loss: 0.5267\n",
      "Epoch [5/17], Loss: 0.4885\n",
      "Epoch [6/17], Loss: 0.4781\n",
      "Epoch [7/17], Loss: 0.4561\n",
      "Epoch [8/17], Loss: 0.4461\n",
      "Epoch [9/17], Loss: 0.4297\n",
      "Epoch [10/17], Loss: 0.4192\n",
      "Epoch [11/17], Loss: 0.4032\n",
      "Epoch [12/17], Loss: 0.4052\n",
      "Epoch [13/17], Loss: 0.4004\n",
      "Epoch [14/17], Loss: 0.3805\n",
      "Epoch [15/17], Loss: 0.3889\n",
      "Epoch [16/17], Loss: 0.3807\n",
      "Epoch [17/17], Loss: 0.3681\n",
      "Test Accuracy: 84.15%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len(train_pca_dataset)\n",
    "for i in train_pca_dataset:\n",
    "    print (i)\n",
    "    break\n",
    "\n",
    "\n",
    "# train(model,train_data_pca)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train(model, train_pca_loader, criterion, optimizer, device, epochs=17, test_loader = test_pca_loader)\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc735a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.15%\n",
      "Train Accuracy: 88.34%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad() :\n",
    "    for images, labels in train_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ccaa0f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LBFGS.__init__() got an unexpected keyword argument 'momentum'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m modelMLPPCA = MLPPCA().to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m optimizer = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLBFGS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelMLPPCA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m train(modelMLPPCA, train_pca_loader, criterion, optimizer, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, epochs=\u001b[32m17\u001b[39m, test_loader = test_pca_loader)\n\u001b[32m     27\u001b[39m metricslearn(modelMLPPCA, train_pca_loader, test_pca_loader, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: LBFGS.__init__() got an unexpected keyword argument 'momentum'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MLPPCA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPPCA, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(117, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "modelMLPPCA = MLPPCA()\n",
    "\n",
    "\n",
    "modelMLPPCA = MLPPCA().to(\"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.LBFGS(modelMLPPCA.parameters(), lr=0.01, momentum=0.9)\n",
    "train(modelMLPPCA, train_pca_loader, criterion, optimizer, \"cpu\", epochs=17, test_loader = test_pca_loader)\n",
    "metricslearn(modelMLPPCA, train_pca_loader, test_pca_loader, \"cpu\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
